{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning for Semantic Segmentation\n",
    "\n",
    "Here we will fine-tune the model for semantic segmentation. This is done by simply adding a new point feature upscaling and classification head to the end of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb8f045b400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from point2vec.datasets import LArNetDataModule\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Turn off gradient tracking so we don't run out of memory\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATASET] self.emin=1e-06, self.emax=20.0, self.energy_threshold=0.13, self.normalize=True, self.remove_low_energy_scatters=True\n",
      "[DATASET] Building index\n",
      "[DATASET] 864064 point clouds were loaded\n",
      "[DATASET] 10 files were loaded\n",
      "[DATASET] self.emin=1e-06, self.emax=20.0, self.energy_threshold=0.13, self.normalize=True, self.remove_low_energy_scatters=True\n",
      "[DATASET] Building index\n",
      "[DATASET] 8531 point clouds were loaded\n",
      "[DATASET] 1 files were loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = LArNetDataModule(\n",
    "    data_path=f'/sdf/home/y/youngsam/data/dune/larnet/h5/DataAccessExamples/train/generic_v2*.h5',\n",
    "    batch_size=24,\n",
    "    num_workers=0,\n",
    "    dataset_kwargs={\n",
    "        'emin': 1.0e-6,                      # min energy for log transform\n",
    "        'emax': 20.0,                        # max energy for log transform\n",
    "        'energy_threshold': 0.13,            # remove points with energy < 0.13\n",
    "        'remove_low_energy_scatters': True,  # remove low energy scatters (PID=4)\n",
    "        'maxlen': -1,                        # max number of events to load\n",
    "        'normalize': True,                   # normalize point cloud to unit sphere\n",
    "    }\n",
    ")\n",
    "dataset.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sdf/home/y/youngsam/sw/dune/.conda/envs/py310/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from point2vec.models import PointMAE\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def get_newest_ckpt(ckpt_path):\n",
    "    ckpt_path = glob(f'{ckpt_path}/**/*.ckpt')\n",
    "    newest_ckpt = max(ckpt_path, key=os.path.getctime)\n",
    "    return newest_ckpt\n",
    "\n",
    "wandb_run_id = 'fjnp0snd'\n",
    "ckpt_path = f'/sdf/home/y/youngsam/sw/dune/representations/point2vec/PointMAE-Pretraining-LArNet-5voxel/{wandb_run_id}'\n",
    "\n",
    "model = PointMAE.load_from_checkpoint(\n",
    "    get_newest_ckpt(ckpt_path)\n",
    ").cuda()\n",
    "\n",
    "# fix the tokenizer, as a bug was fixed in the latest version of the code (see https://github.com/youngsm/point2vec/commit/b32552088422d5210897dd548b3c77fbf1b0c0b5)\n",
    "model.tokenizer.grouping.num_groups = 1024\n",
    "model.tokenizer.grouping.context_length = 640\n",
    "model.tokenizer.grouping.group_size = 32\n",
    "model.tokenizer.grouping.upscale_group_size = 256\n",
    "model.tokenizer.grouping.group_radius = 5 / 760\n",
    "model.tokenizer.grouping.overlap_factor = 0.6\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our point feature upsampler will do the following:\n",
    "\n",
    "\n",
    "Given some embeddings and centers and the points we will want to upscale to, we will\n",
    "\n",
    "1. Find the K nearest embeddings to each center\n",
    "2. Interpolate via inverse distance weighting to get embeddings for each point.\n",
    "3. Apply a 2 layer MLP with batch normalization to the embeddings\n",
    "\n",
    "\n",
    "In practice, the embeddings will actually be the average embeddings of a list of N layers in the encoder. For this we will use N=[3,7,11], and K=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point2vec.modules.masking import masked_layer_norm\n",
    "\n",
    "def get_embeddings(model, points, lengths, seg_head_fetch_layers=[3,7,11]):\n",
    "    point_mask = torch.arange(lengths.max(), device=lengths.device).expand(\n",
    "        len(lengths), -1\n",
    "    ) < lengths.unsqueeze(-1)\n",
    "\n",
    "    tokens, centers, embedding_mask, _, _ = model.tokenizer(points, lengths)\n",
    "    pos = model.positional_encoding(centers[..., :3])\n",
    "    output = model.encoder(tokens, pos, embedding_mask, return_hidden_states=True)\n",
    "    batch_lengths = embedding_mask.sum(dim=1)\n",
    "\n",
    "    hidden_states = [\n",
    "        masked_layer_norm(output.hidden_states[i], output.hidden_states[i].shape[-1], embedding_mask)\n",
    "        for i in seg_head_fetch_layers]  # type: ignore [(B, T, C)]\n",
    "    token_features = torch.stack(hidden_states, dim=0).mean(0)  # (B, T, C)\n",
    "\n",
    "    return token_features, centers, embedding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from point2vec.modules.feature_upsampling import PointNetFeatureUpsampling\n",
    "\n",
    "# Feature Upsampler ============================\n",
    "point_dim = model.hparams.num_channels\n",
    "upsampling_dim = model.hparams.encoder_dim\n",
    "feature_upsampler = PointNetFeatureUpsampling(\n",
    "    in_channel=point_dim + upsampling_dim,\n",
    "    mlp=[upsampling_dim, upsampling_dim],\n",
    ").cuda()\n",
    "\n",
    "points, lengths, labels, _ = next(iter(dataset.train_dataloader()))\n",
    "B, N, C = points.shape\n",
    "points = points.cuda()   # (B, N, 4)\n",
    "lengths = lengths.cuda() # (B,)\n",
    "labels = labels.cuda().squeeze(-1) # (B, N)\n",
    "\n",
    "\n",
    "# Get embeddings\n",
    "point_mask = torch.arange(lengths.max(), device=lengths.device).expand(\n",
    "    len(lengths), -1\n",
    ") < lengths.unsqueeze(-1) # (B, N)\n",
    "\n",
    "embeddings, centers, embedding_mask = get_embeddings(model, points, lengths)\n",
    "group_lengths = embedding_mask.sum(dim=1)\n",
    "\n",
    "upsampled_features,_ = feature_upsampler(\n",
    "    points[..., :3],                 # xyz1\n",
    "    centers[..., :3],       # xyz2\n",
    "    points,                 # points1\n",
    "    embeddings,             # points2\n",
    "    lengths,                # point_lens\n",
    "    group_lengths,          # embedding_lens\n",
    "    point_mask,             # point_mask for masked bn\n",
    ") # (B, N, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled_features.shape torch.Size([24, 6347, 384])\n",
      "points.shape torch.Size([24, 6347, 4])\n"
     ]
    }
   ],
   "source": [
    "print('upsampled_features.shape', upsampled_features.shape)\n",
    "print('points.shape', points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a latent feature for each individual point in the point cloud. Now we can perform point classification by running each point through a classification head.\n",
    "\n",
    "In Point-MAE/poit2vec, we actually concatenate along with the individual point features two global feature vectors that give a per-event summary of the point cloud. These correspond to the maximum and mean of the token features for each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled_features.shape torch.Size([24, 6347, 1152])\n"
     ]
    }
   ],
   "source": [
    "def masked_mean(group, point_mask):\n",
    "    valid_elements = point_mask.sum(-1).float().clamp(min=1)\n",
    "    return (group * point_mask.unsqueeze(-1)).sum(-2) / valid_elements.unsqueeze(-1)\n",
    "\n",
    "def masked_max(group, point_mask):\n",
    "    return (group - 1e10 * (~point_mask.unsqueeze(-1))).max(-2).values\n",
    "\n",
    "B, N, C = points.shape\n",
    "global_feature = torch.cat(\n",
    "    [masked_max(embeddings, embedding_mask), masked_mean(embeddings, embedding_mask)], dim=-1\n",
    ")\n",
    "upsampled_features = torch.cat(\n",
    "    [upsampled_features, global_feature.unsqueeze(-1).expand(-1, -1, N).transpose(1, 2)], dim=-1\n",
    ")\n",
    "print('upsampled_features.shape', upsampled_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final segmentation head will be a 3 layer MLP with batch normalization and dropout. Each layer will downscale the feature dimension by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_seg_classes 4\n",
      "First 10 predictions and labels:\n",
      "tensor([0, 3, 2, 2, 2, 2, 3, 0, 3, 2], device='cuda:0') tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from point2vec.modules.masking import MaskedBatchNorm1d\n",
    "\n",
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim: int,\n",
    "        upsampling_dim: int,\n",
    "        seg_head_dim: int,\n",
    "        seg_head_dropout: float,\n",
    "        num_seg_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            2 * encoder_dim + upsampling_dim,\n",
    "            seg_head_dim,\n",
    "            1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = MaskedBatchNorm1d(seg_head_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(seg_head_dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(seg_head_dim, seg_head_dim // 2, 1, bias=False)\n",
    "        self.bn2 = MaskedBatchNorm1d(seg_head_dim // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # self.dropout2 = nn.Dropout(seg_head_dropout)  # Uncomment if needed\n",
    "\n",
    "        self.conv3 = nn.Conv1d(seg_head_dim // 2, num_seg_classes, 1)\n",
    "\n",
    "    def forward(self, x, point_mask):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape [B, C, N], where N is the maximum number of points.\n",
    "        point_mask: Boolean tensor of shape [B, N], where True indicates valid points.\n",
    "        \"\"\"\n",
    "        # Ensure point_mask has the correct shape and type\n",
    "        mask = point_mask.unsqueeze(1).float()  # [B, 1, N]\n",
    "\n",
    "        # Apply first layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, mask)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Apply second layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, mask)\n",
    "        x = self.relu2(x)\n",
    "        # x = self.dropout2(x)  # Uncomment if dropout is needed\n",
    "\n",
    "        # Final convolution layer (no batch norm or activation)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "seg_head_dim = 512\n",
    "num_seg_classes = dataset.num_seg_classes\n",
    "print('num_seg_classes', num_seg_classes)\n",
    "seg_head_dropout = 0.5\n",
    "\n",
    "segmentation_head = SegmentationHead(\n",
    "    encoder_dim=model.hparams.encoder_dim,\n",
    "    upsampling_dim=upsampling_dim,\n",
    "    seg_head_dim=seg_head_dim,\n",
    "    seg_head_dropout=seg_head_dropout,\n",
    "    num_seg_classes=num_seg_classes,\n",
    ").cuda()\n",
    "\n",
    "cls_logits = segmentation_head(upsampled_features.transpose(1,2), point_mask).transpose(1,2)\n",
    "\n",
    "pred_label = torch.max(cls_logits, dim=-1).indices\n",
    "\n",
    "print('First 10 predictions and labels:')\n",
    "print(pred_label[0, :10], labels.squeeze()[0, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This entire model is encapsulated in `point2vec.models.part_segmentation.Point2VecPartSegmentation`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
